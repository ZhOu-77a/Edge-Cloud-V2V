# import os
# import sys
# import json
# import torch
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# from tqdm import tqdm
# from stream_metrics import calc_clip_score

# current_dir = os.path.dirname(os.path.abspath(__file__))
# parent_dir = os.path.dirname(current_dir)
# sys.path.append(parent_dir)

# # æ˜¾å¡è®¾ç½®
# os.environ["CUDA_VISIBLE_DEVICES"] = "1" 
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# # === é…ç½®åŒºåŸŸ ===
# # åº”è¯¥æ”¹æˆå–å‰ä¿©å¸§å‡ºæ¥çœ‹çœ‹å³å¯
# INPUT_VIDEO_DIR = "/home/zhoujh/Edge-Cloud-diffusion/Dataset/video_2s_1"
# PROMPT_CONFIG_FILE = "/home/zhoujh/Edge-Cloud-diffusion/MyCogVideo-v2v/prompts_config.json"


# OUTPUT_CSV = "source_target_gap_16.csv"
# OUTPUT_PLOT_DIR = "plots_similarity_16"




# if not os.path.exists(OUTPUT_PLOT_DIR):
#     os.makedirs(OUTPUT_PLOT_DIR)

# def main():
#     # 1. è¯»å– Prompts
#     if not os.path.exists(PROMPT_CONFIG_FILE):
#         print(f"âŒ Config file not found: {PROMPT_CONFIG_FILE}")
#         return
#     with open(PROMPT_CONFIG_FILE, 'r') as f:
#         prompts_data = json.load(f)
    
#     # ã€æ–°å¢ã€‘ç¡®ä¿æŒ‰ç…§ ID æ’åºï¼Œå¹¶æå–åç§°åˆ—è¡¨ç”¨äºåç»­ç”»å›¾æ’åº
#     # key=lambda x: x['id'] ä¿è¯äº† prompt åˆ—è¡¨æ˜¯æŒ‰ id 1, 2, 3... é¡ºåºæ’åˆ—çš„
#     prompts_data.sort(key=lambda x: x['id']) 
#     prompt_order_list = [p['name'] for p in prompts_data]
    
#     print(f"ğŸ“– Loaded {len(prompts_data)} prompts.")

#     # 2. è¯»å– Videos
#     if not os.path.exists(INPUT_VIDEO_DIR):
#         print(f"âŒ Video dir not found: {INPUT_VIDEO_DIR}")
#         return
#     video_files = [f for f in os.listdir(INPUT_VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))]
#     video_files.sort()
#     print(f"ğŸ“‚ Found {len(video_files)} videos.")

#     results = []

#     print("\nğŸš€ Starting Similarity Evaluation (Source Video vs Target Prompt)...")
    
#     # 3. åŒé‡å¾ªç¯è®¡ç®—
#     # è¿™é‡Œçš„ CLIP Score ä»£è¡¨ï¼šåŸè§†é¢‘ç”»é¢ ä¸ ç›®æ ‡æ–‡å­—æè¿° çš„ç›¸ä¼¼ç¨‹åº¦
#     # åˆ†æ•°è¶Šä½ = è¯­ä¹‰å·®è·è¶Šå¤§ = ç†è®ºä¸Šç”Ÿæˆéš¾åº¦è¶Šå¤§
#     for video_file in tqdm(video_files, desc="Videos"):
#         video_path = os.path.join(INPUT_VIDEO_DIR, video_file)
#         video_name = os.path.splitext(video_file)[0]
        
#         for p_item in prompts_data:
#             p_id = p_item['id']
#             p_text = p_item['prompt']
#             # p_diff = p_item['difficulty']
            
#             # è®¡ç®— CLIP Score (åªå– text_score, å¿½ç•¥ consistency)
#             # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ˜¯ç”¨ "åŸè§†é¢‘" å»æµ‹ "ç›®æ ‡Prompt"
#             score, _ = calc_clip_score(video_path, p_text)
            
#             results.append({
#                 "video": video_name,
#                 "prompt_id": p_id,
#                 "prompt_name": p_item['name'],
#                 # "difficulty_label": p_diff,
#                 "initial_clip_score": score
#             })

#     # 4. ä¿å­˜æ•°æ®
#     df = pd.DataFrame(results)
#     df.to_csv(OUTPUT_CSV, index=False)
#     print(f"\nğŸ’¾ Similarity scores saved to {OUTPUT_CSV}")

#     # 5. å¯è§†åŒ–
#     # å°†æ’åºå¥½çš„ prompt åå­—åˆ—è¡¨ä¼ è¿›å»
#     plot_analysis(df, prompt_order_list)

# def plot_analysis(df, prompt_order):
#     print("ğŸ“Š Generating plots...")
    
#     # è®¾ç½®ç»˜å›¾é£æ ¼
#     sns.set_theme(style="whitegrid")
    
#     # --- å›¾ 1: çƒ­åŠ›å›¾ (Videos vs Prompts) ---
#     # é¢œè‰²è¶Šå†·(è“/ç´«)ä»£è¡¨ç›¸ä¼¼åº¦è¶Šä½(Gapè¶Šå¤§)ï¼Œé¢œè‰²è¶Šæš–(çº¢)ä»£è¡¨è¶Šç›¸ä¼¼
#     plt.figure(figsize=(16, 10))
    
#     # æ¨ªçºµåæ ‡è°ƒæ¢
#     # index (Yè½´) = video
#     # columns (Xè½´) = prompt_name
#     pivot_table = df.pivot(index="video", columns="prompt_name", values="initial_clip_score")
    
#     # æŒ‰ç…§ JSON ID çš„é¡ºåºæ’åˆ—åˆ—(Columns)
#     # reindex ä¼šæ ¹æ®åˆ—è¡¨é‡Œçš„åå­—é¡ºåºé‡æ–°æ’åˆ—åˆ—
#     pivot_table = pivot_table.reindex(columns=prompt_order)
    
#     # ä½¿ç”¨ 'coolwarm' æˆ– 'RdYlBu_r' è‰²å›¾ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŒºåˆ† é«˜(ç›¸ä¼¼) å’Œ ä½(ä¸ç›¸ä¼¼)
#     # annot_kws={"size": 11, "weight": "bold"} è®©æ ¼å­é‡Œçš„æ•°å­—æ›´æ¸…æ™°
#     ax = sns.heatmap(
#         pivot_table, 
#         annot=True, 
#         fmt=".3f", 
#         cmap="RdYlBu_r", 
#         linewidths=.5,
#         annot_kws={"size": 11}
#     )
    
#     # å­—ä½“å˜å¤§ã€åŠ ç²—
#     plt.title("Initial Semantic Similarity (Source Video vs Target Prompt)", fontsize=20, fontweight='bold', pad=20)
    
#     # åæ ‡è½´æ ‡ç­¾è®¾ç½®
#     plt.xlabel("Target Prompt", fontsize=16, fontweight='bold', labelpad=15)
#     plt.ylabel("Source Video", fontsize=16, fontweight='bold', labelpad=15)
    
#     # åˆ»åº¦æ ‡ç­¾è®¾ç½® 
#     # rotation=30: Xè½´æ–‡å­—å€¾æ–œ30åº¦ï¼Œé˜²æ­¢é‡å 
#     plt.xticks( ha='right', fontsize=12)
#     plt.yticks(fontsize=12)
    
#     plt.tight_layout()
#     save_path = os.path.join(OUTPUT_PLOT_DIR, "heatmap_similarity.png")
#     plt.savefig(save_path)
#     print(f" ğŸ‘‰ Heatmap saved to {save_path}")

# if __name__ == "__main__":
#     main()

import os
import sys
import json
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import cv2
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# æ˜¾å¡è®¾ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 
device = "cuda" if torch.cuda.is_available() else "cpu"

# === é…ç½®åŒºåŸŸ ===
# INPUT_VIDEO_DIR = os.path.join(parent_dir, "asset/batch_videos_new")
# PROMPT_CONFIG_FILE = os.path.join(parent_dir, "prompts_config.json")
INPUT_VIDEO_DIR = "/home/zhoujh/Edge-Cloud-diffusion/Dataset/video_2s_21"
PROMPT_CONFIG_FILE = "/home/zhoujh/Edge-Cloud-diffusion/MyCogVideo-v2v/prompts_config.json"


OUTPUT_CSV = "source_target_gap_19.csv"
OUTPUT_PLOT_DIR = "plots_similarity_19"

if not os.path.exists(OUTPUT_PLOT_DIR):
    os.makedirs(OUTPUT_PLOT_DIR)

# ===  Semantic Clip Score (3å¸§é‡‡æ ·) ===
def calc_clip_score(video_path, prompt, model, processor):
    try:
        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
        video_embs = []
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            print(f"Error opening video: {video_path}")
            return 0.0
        
        # 1. è·å–æ€»å¸§æ•°
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames <= 0:
            cap.release()
            return 0.0
            
        # 2. å‡åŒ€é‡‡æ · 3 å¸§ç´¢å¼•
        target_indices = [0, total_frames // 2, max(0, total_frames - 1)]
        # # è·å–æ‰€æœ‰å¸§çš„ç´¢å¼•
        # target_indices = list(range(total_frames))
        target_indices = sorted(list(set(target_indices))) # å»é‡æ’åº

        text_embeds = None

        # 3. å¾ªç¯è¯»å–è¿™ 3 å¸§
        for frame_idx in target_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                # BGR -> RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                image = Image.fromarray(frame_rgb)
                
                with torch.no_grad():
                    inputs = processor(text=[prompt], images=image, return_tensors="pt", padding=True)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    outputs = model(**inputs)
                    
                    # æ”¶é›† Image Embedding
                    video_embs.append(outputs.image_embeds)
                    text_embeds = outputs.text_embeds
        
        cap.release()

        if not video_embs: 
            return 0.0

        # 4. è®¡ç®—åˆ†æ•°
        video_embs = torch.cat(video_embs, dim=0) # Shape: [3, 512]
        
        text_score = cos(text_embeds, video_embs).mean().cpu().item()

        return text_score

    except Exception as e:
        print(f"Error in CLIP Score: {e}")
        return 0.0

def main():
    # 1. è¯»å– Prompts
    if not os.path.exists(PROMPT_CONFIG_FILE):
        print(f"âŒ Config file not found: {PROMPT_CONFIG_FILE}")
        return
    with open(PROMPT_CONFIG_FILE, 'r') as f:
        prompts_data = json.load(f)
    
    # æ’åº
    prompts_data.sort(key=lambda x: x['id']) 
    prompt_order_list = [p['name'] for p in prompts_data]
    print(f"ğŸ“– Loaded {len(prompts_data)} prompts.")

    # 2. è¯»å– Videos
    if not os.path.exists(INPUT_VIDEO_DIR):
        print(f"âŒ Video dir not found: {INPUT_VIDEO_DIR}")
        return
    video_files = [f for f in os.listdir(INPUT_VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))]
    video_files.sort()
    print(f"ğŸ“‚ Found {len(video_files)} videos.")

    # === åˆå§‹åŒ–æ¨¡å‹ ===
    print("â³ Loading CLIP model (openai/clip-vit-base-patch32)...")
    try:
        model_id = "openai/clip-vit-base-patch32"
        model = CLIPModel.from_pretrained(model_id).to(device)
        processor = CLIPProcessor.from_pretrained(model_id)
        print("âœ… Model loaded.")
    except Exception as e:
        print(f"âŒ Failed to load CLIP model: {e}")
        return

    results = []
    print("\nğŸš€ Starting Similarity Evaluation (Text Score Only)...")
    
    # 3. åŒé‡å¾ªç¯è®¡ç®—
    for video_file in tqdm(video_files, desc="Videos"):
        video_path = os.path.join(INPUT_VIDEO_DIR, video_file)
        video_name = os.path.splitext(video_file)[0]
        
        for p_item in prompts_data:
            p_id = p_item['id']
            p_text = p_item['prompt']
            
            # === ä¿®æ”¹ï¼šåªæ¥æ”¶ä¸€ä¸ªè¿”å›å€¼ ===
            score = calc_clip_score(video_path, p_text, model, processor)
            
            results.append({
                "video": video_name,
                "prompt_id": p_id,
                "prompt_name": p_item['name'],
                "initial_clip_score": score
            })

    # 4. ä¿å­˜æ•°æ®
    df = pd.DataFrame(results)
    df.to_csv(OUTPUT_CSV, index=False)
    print(f"\nğŸ’¾ Similarity scores saved to {OUTPUT_CSV}")

    # 5. å¯è§†åŒ–
    plot_analysis(df, prompt_order_list)

def plot_analysis(df, prompt_order):
    print("ğŸ“Š Generating plots...")
    
    sns.set_theme(style="whitegrid")
    
    plt.figure(figsize=(16, 10))
    
    pivot_table = df.pivot(index="video", columns="prompt_name", values="initial_clip_score")
    pivot_table = pivot_table.reindex(columns=prompt_order)
    
    ax = sns.heatmap(
        pivot_table, 
        annot=True, 
        fmt=".3f", 
        cmap="RdYlBu_r", 
        linewidths=.5,
        annot_kws={"size": 11}
    )
    
    plt.title("Initial Semantic Similarity (Source Video vs Target Prompt)", fontsize=20, fontweight='bold', pad=20)
    plt.xlabel("Target Prompt", fontsize=16, fontweight='bold', labelpad=15)
    plt.ylabel("Source Video", fontsize=16, fontweight='bold', labelpad=15)
    
    plt.xticks(ha='right', fontsize=12, rotation=30)
    plt.yticks(fontsize=12)
    
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_PLOT_DIR, "heatmap_similarity.png")
    plt.savefig(save_path)
    print(f" ğŸ‘‰ Heatmap saved to {save_path}")

if __name__ == "__main__":
    main()