import os
import sys
import torch
import requests
import base64
import numpy as np
import time
import imageio.v3 as iio
from PIL import Image


from videox_fun.models import AutoencoderKLCogVideoX
from videox_fun.utils.utils import get_video_to_video_latent, save_videos_grid

# === è°ƒè¯•å‡½æ•° ===
def print_stats(step_name, tensor):
    if isinstance(tensor, torch.Tensor):
        t = tensor.float().cpu()
        print(f"ğŸ  [DEBUG-EDGE] {step_name:<25} | Mean: {t.mean():.6f} | Std: {t.std():.6f} | Min: {t.min():.4f} | Max: {t.max():.4f}")

# === 3. é…ç½® ===
CLOUD_URL = "http://127.0.0.1:12345/inference"
MODEL_NAME = "models/Diffusion_Transformer/CogVideoX-Fun-V1.1-2b-InP" 
DEVICE = "cuda"
WEIGHT_DTYPE = torch.bfloat16

# INPUT_VIDEO = "asset/building.mp4" 
# PROMPT = "A building in cartoon style."
INPUT_VIDEO = "asset/inpaint_video.mp4" 
PROMPT = "A cute cat."
NEGATIVE_PROMPT = "The video is not of a high quality, low resolution, watermark, distortion."
SAMPLE_SIZE = [384, 672] 
VIDEO_LENGTH = 49        
FPS = 8
STRENGTH = 0.8
SEED = 4
print(f"ğŸ  [Edge] Initializing Client (Seed={SEED})...")

print("ğŸ“¦ Loading VAE...")
vae = AutoencoderKLCogVideoX.from_pretrained(MODEL_NAME, subfolder="vae").to(DEVICE).to(WEIGHT_DTYPE)

def encode_tensor(tensor):
    np_array = tensor.cpu().float().numpy().astype(np.float16)
    return base64.b64encode(np_array.tobytes()).decode('utf-8')

def decode_tensor(b64_str, shape):
    bytes_data = base64.b64decode(b64_str)
    np_array = np.frombuffer(bytes_data, dtype=np.float16)
    return torch.from_numpy(np_array.copy()).reshape(shape).to(DEVICE).to(WEIGHT_DTYPE)

def main():
    if not os.path.exists(INPUT_VIDEO):
        print(f"âŒ Video not found: {INPUT_VIDEO}")
        return

    # 1. é¢„å¤„ç† (ä¸ Benchmark å¯¹é½)
    print(f"ğŸ”„ Preprocessing Video: {INPUT_VIDEO}")
    temporal_compression_ratio = vae.config.temporal_compression_ratio 
    target_video_length = int((VIDEO_LENGTH - 1) // temporal_compression_ratio * temporal_compression_ratio) + 1
    
    # è·å– [0, 1] åƒç´ æ•°æ®
    input_video, input_video_mask, _, _ = get_video_to_video_latent(
        INPUT_VIDEO, 
        video_length=target_video_length, 
        sample_size=SAMPLE_SIZE, 
        validation_video_mask=None, 
        fps=FPS
    )
    input_video = input_video.to(DEVICE).to(WEIGHT_DTYPE)
    print_stats("Input Video (Raw)", input_video)

    # ã€æ ¸å¿ƒä¿®æ­£ 1ã€‘: å½’ä¸€åŒ–åˆ° [-1, 1]
    # åŸºå‡†æµ‹è¯•æ˜¾ç¤º Mean åº”è¯¥æ˜¯ 0.003 å·¦å³ï¼Œå¿…é¡»åšè¿™ä¸€æ­¥
    input_video = 2.0 * input_video - 1.0
    print_stats("Input Video (Norm)", input_video)

    # 2. VAE Encode
    print("ğŸ”„ VAE Encoding...")
    t0 = time.time()
    with torch.no_grad():
        # Encode -> Sample -> Scale (æ—  Shift)
        init_latents = vae.encode(input_video).latent_dist.sample()
        init_latents = init_latents * vae.config.scaling_factor

    print_stats("Encoded Latents", init_latents)
    print(f"â±ï¸  Encode Time: {time.time()-t0:.4f}s")

    # 3. Upload
    payload = {
        "latents_b64": encode_tensor(init_latents),
        "shape": list(init_latents.shape),
        "prompt": PROMPT,
        "negative_prompt": NEGATIVE_PROMPT,
        "strength": STRENGTH,
        "steps": 50,
        "guidance_scale": 6.0,
        "seed": SEED  # ã€æ–°å¢ã€‘å‘é€ç§å­
    }

    print("ğŸš€ Sending to Cloud...")
    try:
        t_start = time.time()
        # resp = requests.post(CLOUD_URL, json=payload)
        session = requests.Session()
        session.trust_env = False  # ç¦æ­¢è¯»å–ç³»ç»Ÿä»£ç†é…ç½®
        resp = session.post(CLOUD_URL, json=payload) # ä½¿ç”¨ session.post
        resp.raise_for_status()
        print(f"â±ï¸  Cloud Time: {time.time()-t_start:.4f}s")
        data = resp.json()
    except Exception as e:
        print(f"âŒ Error: {e}")
        return

    # 4. Decode
    print("ğŸ  [Edge] Decoding...")
    result_b64 = data["result_b64"]
    latents_out = decode_tensor(result_b64, init_latents.shape)
    
    with torch.no_grad():
        # å Scale
        latents_out = latents_out / vae.config.scaling_factor
        video_out = vae.decode(latents_out).sample

    # ã€æ ¸å¿ƒä¿®æ­£ 2ã€‘: åå½’ä¸€åŒ– [-1, 1] -> [0, 1]
    # å¦‚æœä¸åŠ è¿™ä¸€æ­¥ï¼Œé¢œè‰²ä¼šå¼‚å¸¸ (Color Leakage)
    video_out = (video_out / 2.0 + 0.5).clamp(0, 1)
    
    # 5. Save
    print("ğŸ’¾ Saving Video...")
    # å®šä¹‰è¾“å‡ºç›®å½•
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    original_filename = os.path.basename(INPUT_VIDEO)
    save_path = os.path.abspath(os.path.join(output_dir, original_filename))
    
    # è½¬ float32 é˜²æ­¢ numpy æŠ¥é”™
    video_out_cpu = video_out.to(dtype=torch.float32).cpu()
    save_videos_grid(video_out_cpu, save_path, fps=FPS)
    print(f"âœ… Saved to {save_path}")

if __name__ == "__main__":
    main()