import os
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
import sys
import torch
import gc
import math
import json
import pandas as pd
import numpy as np
from PIL import Image
from omegaconf import OmegaConf
from tqdm import tqdm
import lpips
from transformers import CLIPProcessor, CLIPModel

# ================= 0. ç¯å¢ƒè·¯å¾„è®¾ç½® =================
current_file_path = os.path.abspath(__file__)
project_roots = [
    os.path.dirname(current_file_path),
    os.path.dirname(os.path.dirname(current_file_path)),
    os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
]
for project_root in project_roots:
    sys.path.insert(0, project_root) if project_root not in sys.path else None

from videox_fun.models import (AutoencoderKLWan, AutoTokenizer, WanT5EncoderModel, Wan2_2Transformer3DModel)
from videox_fun.utils.fp8_optimization import convert_model_weight_to_float8, convert_weight_dtype_wrapper
from videox_fun.utils.utils import save_videos_grid, filter_kwargs
from diffusers import FlowMatchEulerDiscreteScheduler

# ================= 1. å…¨å±€é…ç½® & æ¨¡å‹è·¯å¾„ =================
MODEL_NAME          = "models/Diffusion_Transformer/Wan2.2-Fun-A14B-InP"
CONFIG_PATH         = "config/wan2.2/wan_civitai_i2v.yaml"
PROMPTS_JSON_PATH   = "/home/zhoujh/Edge-Cloud-diffusion/MyCogVideo-v2v/prompts_config.json"
OUTPUT_ROOT         = "experiment_results_wan2.2_t2v"
CSV_PATH            = os.path.join(OUTPUT_ROOT, "experiment_metrics.csv")
DEVICE              = "cuda"
WEIGHT_DTYPE        = torch.bfloat16

# å›ºå®šç”Ÿæˆå‚æ•°
SAMPLE_SIZE         = [480, 832]
VIDEO_LENGTH        = 17
FPS                 = 16
SEED                = 43
GUIDANCE_SCALE      = 6.0
SHIFT               = 5.0
# NUM_INFERENCE_STEPS å°†åœ¨å¾ªç¯ä¸­åŠ¨æ€æ”¹å˜

# åŠ è½½é…ç½®
config = OmegaConf.load(CONFIG_PATH)

def flush():
    gc.collect()
    torch.cuda.empty_cache()

# ================= 2. æŒ‡æ ‡è®¡ç®—ç±» =================
class MetricsCalculator:
    def __init__(self, device):
        self.device = device
        print("ğŸ“Š Loading Metrics Models (LPIPS & CLIP)...")
        # LPIPS (AlexNet backbone)
        self.lpips_loss = lpips.LPIPS(net='alex').to(device)
        # CLIP
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    def compute_clip_score(self, frames_tensor, prompt_text):
        """
        frames_tensor: [T, C, H, W] in range [0, 1] (CPU or GPU)
        """
        # å–ä¸­é—´å¸§å’Œé¦–å°¾å¸§æ±‚å¹³å‡ï¼Œæˆ–è€…æ¯ä¸€å¸§éƒ½ç®—
        # è¿™é‡Œä¸ºäº†æ•ˆç‡ï¼Œå– 3 å¸§ (é¦–ã€ä¸­ã€å°¾)
        T = frames_tensor.shape[0]
        indices = [0, T//2, T-1]
        selected_frames = frames_tensor[indices] # [3, C, H, W]
        
        # è½¬æ¢ tensor [0,1] -> PIL Images list
        pil_images = [Image.fromarray((f.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)) for f in selected_frames]
        
        inputs = self.clip_processor(text=[prompt_text], images=pil_images, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.clip_model(**inputs)
            logits_per_image = outputs.logits_per_image  # [3, 1]
            score = logits_per_image.mean().item()
            
        return score

    def compute_lpips(self, current_frames, target_frames):
        """
        è®¡ç®—å½“å‰è§†é¢‘ä¸ç›®æ ‡è§†é¢‘ï¼ˆé€šå¸¸æ˜¯Step 50ï¼‰çš„æ„ŸçŸ¥è·ç¦»
        frames: [T, C, H, W] in range [0, 1]
        """
        # LPIPS expects input in range [-1, 1]
        curr = current_frames.to(self.device) * 2.0 - 1.0
        tgt = target_frames.to(self.device) * 2.0 - 1.0
        
        with torch.no_grad():
            # è®¡ç®—æ¯ä¸€å¸§çš„è·ç¦»ç„¶åæ±‚å¹³å‡
            dist = self.lpips_loss(curr, tgt) # [T, 1, 1, 1]
            avg_dist = dist.mean().item()
            
        return avg_dist

# ================= 3. æ ¸å¿ƒç”Ÿæˆå‡½æ•° =================
def generate_one_video(prompt, negative_prompt, steps, save_path):
    """
    è¿è¡Œä¸€æ¬¡å®Œæ•´çš„ç”Ÿæˆæµç¨‹å¹¶è¿”å›ç”Ÿæˆçš„å¸§ Tensor (CPU)
    """
    print(f"\nğŸ¬ Generating: Steps={steps} | Prompt={prompt[:30]}...")
    
    # --- 1. Text Encoding ---
    tokenizer = AutoTokenizer.from_pretrained(
        os.path.join(MODEL_NAME, config['text_encoder_kwargs'].get('tokenizer_subpath', 'tokenizer'))
    )
    text_encoder = WanT5EncoderModel.from_pretrained(
        os.path.join(MODEL_NAME, config['text_encoder_kwargs'].get('text_encoder_subpath', 'text_encoder')),
        additional_kwargs=OmegaConf.to_container(config['text_encoder_kwargs']),
        low_cpu_mem_usage=True,
        torch_dtype=WEIGHT_DTYPE,
    ).to(DEVICE).eval()

    def get_prompt_embeds(prompt_str, max_len=512):
        text_inputs = tokenizer(
            [prompt_str], padding="max_length", max_length=max_len, truncation=True,
            add_special_tokens=True, return_tensors="pt",
        )
        embeds = text_encoder(text_inputs.input_ids.to(DEVICE), attention_mask=text_inputs.attention_mask.to(DEVICE))[0]
        seq_len = text_inputs.attention_mask.gt(0).sum(dim=1).long()[0]
        return embeds[0, :seq_len]

    with torch.no_grad():
        context_prompt = get_prompt_embeds(prompt)
        context_neg = get_prompt_embeds(negative_prompt)
        if GUIDANCE_SCALE > 1.0:
            context_input = [context_neg.cpu(), context_prompt.cpu()]
            context_input = [t.to(DEVICE) for t in context_input]
        else:
            context_input = [context_prompt.to(DEVICE)]

    del tokenizer, text_encoder
    flush()

    # --- 2. Latents & Scheduler ---
    temporal_compression_ratio = 4
    spatial_compression_ratio = 8
    latent_channels = 16
    target_frames = (VIDEO_LENGTH - 1) // temporal_compression_ratio + 1
    target_height = SAMPLE_SIZE[0] // spatial_compression_ratio
    target_width  = SAMPLE_SIZE[1] // spatial_compression_ratio

    generator = torch.Generator(device=DEVICE).manual_seed(SEED)
    latents = torch.randn(
        (1, latent_channels, target_frames, target_height, target_width),
        generator=generator, device=DEVICE, dtype=WEIGHT_DTYPE
    )

    scheduler = FlowMatchEulerDiscreteScheduler(
        **filter_kwargs(FlowMatchEulerDiscreteScheduler, OmegaConf.to_container(config['scheduler_kwargs']))
    )
    scheduler.set_timesteps(steps, device=DEVICE)
    timesteps = scheduler.timesteps

    # --- 3. Split Steps (High/Low Noise) ---
    boundary = config['transformer_additional_kwargs'].get('boundary', 0.900)
    boundary_val = boundary * 1000
    phase1_steps = [(i, t) for i, t in enumerate(timesteps) if t >= boundary_val]
    phase2_steps = [(i, t) for i, t in enumerate(timesteps) if t < boundary_val]

    # Condition Y
    with torch.no_grad():
        y_input = torch.zeros((1, 20, target_frames, target_height, target_width), device=DEVICE, dtype=WEIGHT_DTYPE)
        y_model_input = torch.cat([y_input] * 2) if GUIDANCE_SCALE > 1.0 else y_input
        seq_len_trans = math.ceil((target_height * target_width) / (2 * 2) * target_frames)

    # --- 4. Phase 1 Inference ---
    if len(phase1_steps) > 0:
        transformer_2 = Wan2_2Transformer3DModel.from_pretrained(
            os.path.join(MODEL_NAME, config['transformer_additional_kwargs'].get('transformer_high_noise_model_subpath', 'transformer')),
            transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
            low_cpu_mem_usage=True, torch_dtype=WEIGHT_DTYPE,
        )
        convert_model_weight_to_float8(transformer_2, exclude_module_name=["modulation",], device=DEVICE)
        convert_weight_dtype_wrapper(transformer_2, WEIGHT_DTYPE)
        transformer_2.freqs = transformer_2.freqs.to(DEVICE)
        transformer_2.to(DEVICE).eval()

        for i, t in phase1_steps:
            latent_model_input = torch.cat([latents] * 2) if GUIDANCE_SCALE > 1.0 else latents
            timestep = t.expand(latent_model_input.shape[0])
            with torch.no_grad():
                noise_pred = transformer_2(x=latent_model_input, context=context_input, t=timestep, seq_len=seq_len_trans, y=y_model_input)
            if GUIDANCE_SCALE > 1.0:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)
            latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]

        del transformer_2
        flush()

    # --- 5. Phase 2 Inference ---
    if len(phase2_steps) > 0:
        transformer = Wan2_2Transformer3DModel.from_pretrained(
            os.path.join(MODEL_NAME, config['transformer_additional_kwargs'].get('transformer_low_noise_model_subpath', 'transformer')),
            transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
            low_cpu_mem_usage=True, torch_dtype=WEIGHT_DTYPE,
        )
        convert_model_weight_to_float8(transformer, exclude_module_name=["modulation",], device=DEVICE)
        convert_weight_dtype_wrapper(transformer, WEIGHT_DTYPE)
        transformer.freqs = transformer.freqs.to(DEVICE)
        transformer.to(DEVICE).eval()

        for i, t in phase2_steps:
            latent_model_input = torch.cat([latents] * 2) if GUIDANCE_SCALE > 1.0 else latents
            timestep = t.expand(latent_model_input.shape[0])
            with torch.no_grad():
                noise_pred = transformer(x=latent_model_input, context=context_input, t=timestep, seq_len=seq_len_trans, y=y_model_input)
            if GUIDANCE_SCALE > 1.0:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)
            latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]

        del transformer
        flush()

    # --- 6. Decode ---
    vae = AutoencoderKLWan.from_pretrained(
        os.path.join(MODEL_NAME, config['vae_kwargs'].get('vae_subpath', 'vae')),
        additional_kwargs=OmegaConf.to_container(config['vae_kwargs']),
    ).to(DEVICE).to(WEIGHT_DTYPE)

    with torch.no_grad():
        frames = vae.decode(latents).sample
        frames = (frames / 2 + 0.5).clamp(0, 1)
        frames = frames.cpu().float() # æ­¤æ—¶å½¢çŠ¶å·²ç»æ˜¯ [B, C, T, H, W] å³ [1, 3, 17, 480, 832]

    del vae
    flush()

    save_videos_grid(frames, save_path, fps=FPS)
    
    # è¿”å›æ ¼å¼è°ƒæ•´ä¸º [T, C, H, W] ä»¥ä¾¿åé¢è®¡ç®— CLIP/LPIPS
    # frames[0] å–å‡º batch ç»´åº¦ -> [3, 17, H, W]
    # .permute(1, 0, 2, 3) è°ƒæ•´ç»´åº¦ -> [17, 3, H, W]
    return frames[0].permute(1, 0, 2, 3)

# ================= 4. ä¸»ç¨‹åº =================
if __name__ == "__main__":
    # 1. å‡†å¤‡ç›®å½•å’ŒCSV
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    if not os.path.exists(CSV_PATH):
        df = pd.DataFrame(columns=["prompt_id", "prompt_name", "step", "clip_score", "lpips_score_vs_step50"])
        df.to_csv(CSV_PATH, index=False)
    
    # 2. åŠ è½½ Prompts
    with open(PROMPTS_JSON_PATH, 'r') as f:
        prompts_data = json.load(f)

    # 3. åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    # æ³¨æ„ï¼šå¦‚æœæ˜¾å­˜éå¸¸ç´§å¼ ï¼ˆå°äº24Gï¼‰ï¼Œæ­¤å¤„åˆå§‹åŒ–å¯èƒ½ä¼šå ç”¨æ˜¾å­˜å¯¼è‡´åé¢ç”Ÿæˆ OOMã€‚
    # å¦‚æœé‡åˆ° OOMï¼Œéœ€è¦æŠŠ MetricsCalculator çš„åˆå§‹åŒ–æ”¾åˆ°ç”Ÿæˆå¾ªç¯å†…éƒ¨ï¼ˆç”ŸæˆååŠ è½½ï¼Œç®—å®Œåˆ é™¤ï¼‰
    metrics_calc = MetricsCalculator(DEVICE)

    # 4. å¼€å§‹å®éªŒå¾ªç¯
    for item in prompts_data:
        p_id = item['id']
        p_name = item['name']
        prompt = item['prompt']
        neg_prompt = item['negative_prompt']
        
        # åˆ›å»º prompt å¯¹åº”çš„æ–‡ä»¶å¤¹
        safe_name = "".join([c if c.isalnum() else "_" for c in p_name])
        folder_name = f"{p_id}_{safe_name}"
        save_dir = os.path.join(OUTPUT_ROOT, folder_name)
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\n{'='*20}\nProcessing Prompt ID: {p_id} ({p_name})\n{'='*20}")
        
        # ä¸´æ—¶å­˜å‚¨ç”Ÿæˆç»“æœä»¥ä¾¿è®¡ç®— LPIPS
        # key: step, value: frames_tensor (CPU)
        step_frames_cache = {} 
        
        # Step å¾ªç¯ 1 åˆ° 50
        for step in range(1, 51):
            video_filename = f"step_{step:03d}.mp4"
            save_path = os.path.join(save_dir, video_filename)
            
            # A. ç”Ÿæˆè§†é¢‘
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è·³è¿‡ï¼Ÿ(å¯é€‰)
            # if os.path.exists(save_path): continue 
            
            frames = generate_one_video(prompt, neg_prompt, step, save_path)
            step_frames_cache[step] = frames
            
            # B. è®¡ç®— CLIP Score (ä¸éœ€è¦å‚è€ƒè§†é¢‘)
            clip_s = metrics_calc.compute_clip_score(frames, prompt)
            print(f" -> Step {step} CLIP Score: {clip_s:.4f}")
            
            # C. è®°å½•æ•°æ® (LPIPS ç¨åè®¡ç®—)
            # å…ˆå ä½
            new_row = {
                "prompt_id": p_id,
                "prompt_name": p_name,
                "step": step,
                "clip_score": clip_s,
                "lpips_score_vs_step50": None # å¾…å¡«
            }
            
            # è¿½åŠ å†™å…¥ï¼ˆä¸ºäº†é˜²æ­¢ç¨‹åºä¸­é€”å´©æ‰ï¼Œæˆ‘ä»¬å…ˆå†™ä¸€è¡Œï¼Œlpips åé¢ updateï¼‰
            # ä½†ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬ç­‰ Step 50 è·‘å®Œå†ä¸€æ¬¡æ€§è®¡ç®— LPIPS å¹¶å†™å…¥ CSV æ¯”è¾ƒæ•´æ´
            
        # D. Step 50 è·‘å®Œåï¼Œè®¡ç®— LPIPS å¹¶å†™å…¥ CSV
        print(f"âœ… Finished 1-50 steps for prompt {p_id}. Computing LPIPS...")
        
        reference_frames = step_frames_cache[50] # è·å– Step 50 çš„ç»“æœä½œä¸º Ground Truth
        
        results_list = []
        for step in range(1, 51):
            frames = step_frames_cache[step]
            
            # è®¡ç®— LPIPS (å½“å‰ step vs step 50)
            if step == 50:
                lpips_s = 0.0
            else:
                lpips_s = metrics_calc.compute_lpips(frames, reference_frames)
            
            # é‡æ–°è®¡ç®—ä¸€é CLIP (æˆ–è€…ä»ä¸Šé¢å­˜ä¸‹æ¥ï¼Œè¿™é‡Œä¸ºäº†é€»è¾‘ç®€å•é‡æ–°è°ƒä¸€éè®¡ç®—å‡½æ•°å…¶å®å¾ˆå¿«)
            # å®é™…ä¸Šä¸Šé¢å·²ç»ç®—è¿‡äº†ï¼Œè¿™é‡Œæˆ‘ä»¬å‡è®¾ä¸Šé¢åªæ˜¯æ‰“å°ã€‚
            # ä¸ºäº†ä»£ç æ•´æ´ï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¾ªç¯é‡Œå…¶å®åº”è¯¥å­˜åˆ°ä¸€ä¸ª list é‡Œã€‚
            
            # ä¿®æ­£é€»è¾‘ï¼šæˆ‘ä»¬ç”¨ä¸€ä¸ª list å­˜ç»“æœ
            clip_s = metrics_calc.compute_clip_score(frames, prompt) # ä¹Ÿå¯ä»¥ä¸Šé¢å­˜dicté‡Œå–
            
            results_list.append({
                "prompt_id": p_id,
                "prompt_name": p_name,
                "step": step,
                "clip_score": clip_s,
                "lpips_score_vs_step50": lpips_s
            })
            
            print(f"Step {step}: CLIP={clip_s:.4f}, LPIPS={lpips_s:.4f}")

        # E. å†™å…¥ CSV
        batch_df = pd.DataFrame(results_list)
        batch_df.to_csv(CSV_PATH, mode='a', header=False, index=False)
        print(f"ğŸ’¾ Metrics saved to {CSV_PATH}")
        
        # F. æ¸…ç†å†…å­˜ï¼Œé˜²æ­¢ prompt ä¹‹é—´å†…å­˜æ³„æ¼
        del step_frames_cache
        gc.collect()

    print("\nğŸ‰ All experiments finished!")