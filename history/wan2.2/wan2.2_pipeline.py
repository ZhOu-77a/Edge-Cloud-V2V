import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
import sys
import torch
import gc
import math
import numpy as np
from PIL import Image
import cv2  
from omegaconf import OmegaConf
# è¿™ä¸€ç‰ˆæŒ‡å®šäº†è¾“å…¥è§†é¢‘çš„é•¿åº¦



current_file_path = os.path.abspath(__file__)
project_roots = [
    os.path.dirname(current_file_path),
    os.path.dirname(os.path.dirname(current_file_path)),
    os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))
]
for project_root in project_roots:
    sys.path.insert(0, project_root) if project_root not in sys.path else None

from videox_fun.models import (AutoencoderKLWan, AutoTokenizer, WanT5EncoderModel, Wan2_2Transformer3DModel)
from videox_fun.utils.fp8_optimization import convert_model_weight_to_float8, convert_weight_dtype_wrapper
from videox_fun.utils.utils import save_videos_grid, filter_kwargs
from diffusers import FlowMatchEulerDiscreteScheduler

# ================= 1. å‚æ•°é…ç½® =================
MODEL_NAME          = "models/Diffusion_Transformer/Wan2.2-Fun-A14B-InP"
CONFIG_PATH         = "config/wan2.2/wan_civitai_i2v.yaml"
DEVICE              = "cuda"
WEIGHT_DTYPE        = torch.bfloat16

# --- æ–°å¢/ä¿®æ”¹ V2V å‚æ•° ---
# INPUT_VIDEO_PATH    = "asset/inpaint_video.mp4" 
# PROMPT              = "ä¸€åªç™½è‰²çš„çŒ«" 
# NEGATIVE_PROMPT     = "è‰²è°ƒè‰³ä¸½ï¼Œç•¸å½¢ï¼Œæ¨¡ç³Šï¼Œæ‰­æ›²çš„è„¸ï¼Œä¸å®Œæ•´çš„æ‰‹"
INPUT_VIDEO_PATH    = "asset/scene_021_left-forward.mp4" 
PROMPT              = "A video of streetview in Minecraft voxel style, made of cube blocks, low poly, pixelated textures, blocky trees, high quality, detailed." 
NEGATIVE_PROMPT     = "curves, round, high poly, low quality, blurry, distortion."
SAMPLE_SIZE         = [480, 832] # æœ€å¥½å’ŒåŸè§†é¢‘æ¯”ä¾‹ä¸€è‡´
VIDEO_LENGTH        = 17         # è¦ç”Ÿæˆçš„å¸§æ•°
FPS                 = 16
SEED                = 43
GUIDANCE_SCALE      = 6.0
NUM_INFERENCE_STEPS = 50

STRENGTH            = 0.5

config = OmegaConf.load(CONFIG_PATH)

def flush():
    gc.collect()
    torch.cuda.empty_cache()

# ================= 1.5 é¢„å¤„ç†ï¼šåŠ è½½è§†é¢‘å¹¶ç”¨ VAE ç¼–ç  =================
# V2V éœ€è¦å…ˆåŠ è½½ VAE æ¥â€œçœ‹â€åŸè§†é¢‘ï¼Œè¿™æ¯” Text Encoding æ›´æ—©
print("ğŸ  [Pre-Processing] Encoding Input Video...")

def load_video_frames(video_path, frames_num, height, width):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while len(frames) < frames_num:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = cv2.resize(frame, (width, height))
        # å½’ä¸€åŒ–åˆ° [-1, 1] ç”¨äº VAE
        frame = (frame.astype(np.float32) / 127.5) - 1.0 
        frames.append(frame)
    cap.release()
    
    # å¦‚æœè§†é¢‘ä¸å¤Ÿé•¿ï¼Œå¤åˆ¶æœ€åä¸€å¸§ï¼ˆæˆ–è€…ä½ å¯ä»¥åšå¾ªç¯ï¼‰
    while len(frames) < frames_num:
        frames.append(frames[-1])
        
    # [T, H, W, C] -> [C, T, H, W] -> Batch [1, C, T, H, W]
    video_tensor = torch.from_numpy(np.stack(frames)).permute(3, 0, 1, 2).unsqueeze(0)
    return video_tensor.to(DEVICE).to(WEIGHT_DTYPE)

# åŠ è½½ VAE
vae = AutoencoderKLWan.from_pretrained(
    os.path.join(MODEL_NAME, config['vae_kwargs'].get('vae_subpath', 'vae')),
    additional_kwargs=OmegaConf.to_container(config['vae_kwargs']),
).to(DEVICE).to(WEIGHT_DTYPE)

# ç¼–ç åŸè§†é¢‘ -> Latents
with torch.no_grad():
    input_video_tensor = load_video_frames(INPUT_VIDEO_PATH, VIDEO_LENGTH, SAMPLE_SIZE[0], SAMPLE_SIZE[1])
    # VAE ç¼–ç 
    init_latents = vae.encode(input_video_tensor).latent_dist.sample()
    # Wan VAE ç¼©æ”¾å› å­é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨ä¹˜ï¼Œä½†åœ¨ Diffusers ä¸­æœ‰æ—¶éœ€è¦ mapï¼Œè¿™é‡Œå‡è®¾ videox_fun å†…éƒ¨å¤„ç†äº†
    # å¦‚æœç”Ÿæˆçš„ç”»é¢åè‰²æˆ–å…¨é»‘ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è¿™é‡Œçš„ scaling factor

print(f"âœ… Input video encoded. Latents shape: {init_latents.shape}")

del vae
flush()

# ================= 2. äº‘ç«¯ä¾§é€»è¾‘ï¼šæ–‡æœ¬ç¼–ç  (ä¿æŒä¸å˜) =================
print("â˜ï¸ [Cloud] Text Encoding...")
# ... (æ­¤å¤„ä»£ç ä¸ä½ åŸæ¥çš„ä¸€è‡´ï¼Œçœç•¥ä»¥èŠ‚çœç¯‡å¹…) ...
# è¯·æŠŠåŸæ¥çš„ Text Encoding ä»£ç æ®µå®Œæ•´ä¿ç•™åœ¨è¿™é‡Œ
tokenizer = AutoTokenizer.from_pretrained(os.path.join(MODEL_NAME, config['text_encoder_kwargs'].get('tokenizer_subpath', 'tokenizer')))
text_encoder = WanT5EncoderModel.from_pretrained(os.path.join(MODEL_NAME, config['text_encoder_kwargs'].get('text_encoder_subpath', 'text_encoder')), additional_kwargs=OmegaConf.to_container(config['text_encoder_kwargs']), low_cpu_mem_usage=True, torch_dtype=WEIGHT_DTYPE).to(DEVICE).eval()
def get_prompt_embeds(prompt_str, max_len=512):
    text_inputs = tokenizer([prompt_str], padding="max_length", max_length=max_len, truncation=True, add_special_tokens=True, return_tensors="pt")
    text_input_ids = text_inputs.input_ids.to(DEVICE)
    prompt_attention_mask = text_inputs.attention_mask.to(DEVICE)
    embeds = text_encoder(text_input_ids, attention_mask=prompt_attention_mask)[0]
    seq_len = prompt_attention_mask.gt(0).sum(dim=1).long()[0]
    return embeds[0, :seq_len]
with torch.no_grad():
    context_prompt = get_prompt_embeds(PROMPT)
    context_neg = get_prompt_embeds(NEGATIVE_PROMPT)
    if GUIDANCE_SCALE > 1.0:
        context_input = [context_neg.cpu(), context_prompt.cpu()]
        context_input = [t.to(DEVICE) for t in context_input]
    else:
        context_input = [context_prompt.to(DEVICE)]
del tokenizer, text_encoder
flush()

print("â˜ï¸ [Cloud] Preparing Latents for V2V...")

temporal_compression_ratio = 4 
spatial_compression_ratio = 8 


target_frames = init_latents.shape[2]
target_height = init_latents.shape[3]
target_width  = init_latents.shape[4]

scheduler = FlowMatchEulerDiscreteScheduler(
    **filter_kwargs(FlowMatchEulerDiscreteScheduler, OmegaConf.to_container(config['scheduler_kwargs']))
)
scheduler.set_timesteps(NUM_INFERENCE_STEPS, device=DEVICE)
timesteps = scheduler.timesteps

# ã€æ ¸å¿ƒ V2V é€»è¾‘ã€‘ï¼šè®¡ç®—å¼€å§‹æ­¥æ•°
# å‡è®¾ STRENGTH = 0.6ï¼Œæ€»æ­¥æ•° 50ã€‚
# æˆ‘ä»¬éœ€è¦è·³è¿‡å‰ 20 æ­¥ (ä¿ç•™ 40% åŸå›¾ä¿¡æ¯)ï¼Œä»ç¬¬ 20 æ­¥å¼€å§‹å»å™ª (é‡ç»˜ 60%)ã€‚
# FlowMatch çš„ timesteps é€šå¸¸æ˜¯ä» 1000 -> 0 æˆ–è€…æ˜¯ 0 -> 1000ï¼Œå–å†³äºå…·ä½“å®ç°ã€‚
# Wan ä½¿ç”¨ FlowMatchEulerï¼Œé€šå¸¸æ˜¯ time=1.0 (çº¯å™ª) -> time=0.0 (çº¯å›¾)ã€‚

# æ‰¾åˆ°åˆ‡å…¥çš„æ—¶é—´æ­¥
start_timestep_index = int((1.0 - STRENGTH) * NUM_INFERENCE_STEPS)
# ç¡®ä¿ä¸è¶Šç•Œ
start_timestep_index = max(0, min(start_timestep_index, NUM_INFERENCE_STEPS - 1))

start_timestep = timesteps[start_timestep_index]
print(f"âš¡ V2V Mode: Strength {STRENGTH}. Starting from step {start_timestep_index+1} (t={start_timestep:.2f})")

# æˆªå–å‰©ä½™çš„æ—¶é—´æ­¥
timesteps = timesteps[start_timestep_index:]

# ç»™åŸå›¾ Latents åŠ å™ª
generator = torch.Generator(device=DEVICE).manual_seed(SEED)
noise = torch.randn(init_latents.shape, generator=generator, device=DEVICE, dtype=WEIGHT_DTYPE)

# FlowMatch çš„åŠ å™ªå…¬å¼é€šå¸¸æ˜¯çº¿æ€§çš„ï¼š x_t = (1 - t) * x_0 + t * noise (ä¸åŒ scheduler å®ç°ä¸åŒ)
# è¿™é‡Œç›´æ¥ä½¿ç”¨ scheduler çš„ add_noise æ–¹æ³•æœ€ç¨³å¦¥
# æ³¨æ„ï¼šFlowMatchEulerDiscreteScheduler çš„ add_noise å¯èƒ½éœ€è¦ sigmasï¼Œ
# ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ¨¡æ‹Ÿ Flow Matching çš„åŠ å™ªï¼ˆå‡è®¾ t ä¹Ÿå°±æ˜¯ sigmaï¼‰ï¼š
# x_t = t * x_1 + (1 - t) * x_0  <-- Flow Matching æ’å€¼å…¬å¼
# t æ˜¯å½“å‰æ—¶é—´æ­¥çš„å€¼ (ä¾‹å¦‚ 0.6)
t_val = start_timestep.cpu().item() / 1000.0 # å‡è®¾ timestep æ˜¯ 0-1000
# å¦‚æœ timestep æœ¬èº«å°±æ˜¯ 0-1 (FlowMatch å¸¸è§)ï¼Œåˆ™ä¸éœ€è¦é™¤ 1000
if timesteps[0] <= 1.0: 
    t_val = start_timestep.cpu().item()
else:
    t_val = start_timestep.cpu().item() / 1000.0

# æ„é€ èµ·å§‹ Latents
# t=1 æ˜¯å…¨å™ªï¼Œt=0 æ˜¯åŸå›¾ã€‚æˆ‘ä»¬è¦ä» t=STRENGTH å¼€å§‹ã€‚
latents = (1 - t_val) * init_latents + t_val * noise

# ================= 4. Transformer æ¨ç† (ä¿æŒå¤§éƒ¨åˆ†é€»è¾‘) =================
# Condition ä¾ç„¶å…¨ 0ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ç”¨ latents æœ¬èº«å¸¦çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ Inpainting Mask
with torch.no_grad():
    y_input = torch.zeros(
        (1, 20, target_frames, target_height, target_width), 
        device=DEVICE, dtype=WEIGHT_DTYPE
    )
    y_model_input = torch.cat([y_input] * 2) if GUIDANCE_SCALE > 1.0 else y_input
    patch_size = (1, 2, 2)
    seq_len = math.ceil((target_height * target_width) / (patch_size[1] * patch_size[2]) * target_frames)

# åˆ†æ®µé€»è¾‘é‡ç®— (å› ä¸º timesteps å˜çŸ­äº†)
phase1_steps = []
phase2_steps = []
boundary = config['transformer_additional_kwargs'].get('boundary', 0.900)
boundary_val = boundary * 1000 if timesteps[0] > 1.0 else boundary

for i, t in enumerate(timesteps):
    if t >= boundary_val:
        phase1_steps.append((i, t))
    else:
        phase2_steps.append((i, t))

print(f" -> Plan: {len(phase1_steps)} High-Noise steps, {len(phase2_steps)} Low-Noise steps.")

# --- Phase 1: High Noise Model ---
if len(phase1_steps) > 0:
    print("ğŸš€ [Phase 1] Loading High Noise Transformer...")
    # ... (åŠ è½½ Transformer 2 ä»£ç åŒåŸç‰ˆ) ...
    transformer_2 = Wan2_2Transformer3DModel.from_pretrained(
        os.path.join(MODEL_NAME, config['transformer_additional_kwargs'].get('transformer_high_noise_model_subpath', 'transformer')),
        transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
        low_cpu_mem_usage=True, torch_dtype=WEIGHT_DTYPE,
    )
    convert_model_weight_to_float8(transformer_2, exclude_module_name=["modulation",], device=DEVICE)
    convert_weight_dtype_wrapper(transformer_2, WEIGHT_DTYPE)
    transformer_2.freqs = transformer_2.freqs.to(DEVICE)
    transformer_2.to(DEVICE).eval()
    
    for i, t in phase1_steps:
        # è¿™é‡Œçš„ loop é€»è¾‘ä¿æŒä¸å˜
        latent_model_input = torch.cat([latents] * 2) if GUIDANCE_SCALE > 1.0 else latents
        timestep = t.expand(latent_model_input.shape[0])
        with torch.no_grad():
            noise_pred = transformer_2(x=latent_model_input, context=context_input, t=timestep, seq_len=seq_len, y=y_model_input)
        if GUIDANCE_SCALE > 1.0:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)
        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]
        print(f"Step {i+1} done.")
    
    del transformer_2
    flush()

# --- Phase 2: Low Noise Model ---
if len(phase2_steps) > 0:
    print("ğŸš€ [Phase 2] Loading Low Noise Transformer...")
    # ... (åŠ è½½ Transformer 1 ä»£ç åŒåŸç‰ˆ) ...
    transformer = Wan2_2Transformer3DModel.from_pretrained(
        os.path.join(MODEL_NAME, config['transformer_additional_kwargs'].get('transformer_low_noise_model_subpath', 'transformer')),
        transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
        low_cpu_mem_usage=True, torch_dtype=WEIGHT_DTYPE,
    )
    convert_model_weight_to_float8(transformer, exclude_module_name=["modulation",], device=DEVICE)
    convert_weight_dtype_wrapper(transformer, WEIGHT_DTYPE)
    transformer.freqs = transformer.freqs.to(DEVICE)
    transformer.to(DEVICE).eval()

    for i, t in phase2_steps:
        latent_model_input = torch.cat([latents] * 2) if GUIDANCE_SCALE > 1.0 else latents
        timestep = t.expand(latent_model_input.shape[0])
        with torch.no_grad():
            noise_pred = transformer(x=latent_model_input, context=context_input, t=timestep, seq_len=seq_len, y=y_model_input)
        if GUIDANCE_SCALE > 1.0:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)
        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]
        print(f"Step {i+1} done.")

    del transformer
    flush()

# ================= 5. è§£ç  (é‡æ–°åŠ è½½ VAE) =================
print("ğŸ  [Edge] Decoding...")
vae = AutoencoderKLWan.from_pretrained(
    os.path.join(MODEL_NAME, config['vae_kwargs'].get('vae_subpath', 'vae')),
    additional_kwargs=OmegaConf.to_container(config['vae_kwargs']),
).to(DEVICE).to(WEIGHT_DTYPE)

with torch.no_grad():
    frames = vae.decode(latents).sample
    frames = (frames / 2 + 0.5).clamp(0, 1)
    frames = frames.cpu().float()

save_path = "samples/output_v2v_streetview3.mp4"
save_videos_grid(frames, save_path, fps=FPS)
print(f"âœ… Video saved to: {save_path}")